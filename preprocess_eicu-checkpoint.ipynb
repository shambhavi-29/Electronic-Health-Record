{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c6b2caa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import tensorflow as tf\n",
    "tf.compat.v1.enable_eager_execution()\n",
    "import sys\n",
    "import pickle\n",
    "import argparse\n",
    "import numpy as np\n",
    "from sklearn import model_selection\n",
    "from scipy.sparse import csr_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2be874b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncounterInfo(object):\n",
    "\n",
    "    def __init__(self, patient_id, encounter_id, encounter_timestamp,\n",
    "                 readmission):\n",
    "        self.patient_id = patient_id\n",
    "        self.encounter_id = encounter_id\n",
    "        self.encounter_timestamp = encounter_timestamp\n",
    "        self.readmission = readmission\n",
    "        self.dx_ids = []\n",
    "        self.rx_ids = []\n",
    "        self.labs = {}\n",
    "        self.physicals = []\n",
    "        self.treatments = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "88a1135d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_patient(infile, encounter_dict, hour_threshold=24):\n",
    "    inff = open(infile, 'r')\n",
    "    count = 0\n",
    "    patient_dict = {}\n",
    "    for line in csv.DictReader(inff):\n",
    "        if count % 10000 == 0:\n",
    "            sys.stdout.write('%d\\r' % count)\n",
    "            sys.stdout.flush()\n",
    "\n",
    "        patient_id = line['patienthealthsystemstayid']\n",
    "        encounter_id = line['patientunitstayid']\n",
    "        encounter_timestamp = -int(line['hospitaladmitoffset'])\n",
    "        if patient_id not in patient_dict:\n",
    "            patient_dict[patient_id] = []\n",
    "        patient_dict[patient_id].append((encounter_timestamp, encounter_id))\n",
    "    inff.close()\n",
    "    print('')\n",
    "\n",
    "    patient_dict_sorted = {}\n",
    "    for patient_id, time_enc_tuples in patient_dict.items():\n",
    "        patient_dict_sorted[patient_id] = sorted(time_enc_tuples)\n",
    "\n",
    "    enc_readmission_dict = {}\n",
    "    for patient_id, time_enc_tuples in patient_dict_sorted.items():\n",
    "        for time_enc_tuple in time_enc_tuples[:-1]:\n",
    "            enc_id = time_enc_tuple[1]\n",
    "            enc_readmission_dict[enc_id] = True\n",
    "        last_enc_id = time_enc_tuples[-1][1]\n",
    "        enc_readmission_dict[last_enc_id] = False\n",
    "\n",
    "    inff = open(infile, 'r')\n",
    "    count = 0\n",
    "    for line in csv.DictReader(inff):\n",
    "        if count % 10000 == 0:\n",
    "            sys.stdout.write('%d\\r' % count)\n",
    "            sys.stdout.flush()\n",
    "\n",
    "        patient_id = line['patienthealthsystemstayid']\n",
    "        encounter_id = line['patientunitstayid']\n",
    "        encounter_timestamp = -int(line['hospitaladmitoffset'])\n",
    "        discharge_status = line['unitdischargestatus']\n",
    "        duration_minute = float(line['unitdischargeoffset'])\n",
    "        readmission = enc_readmission_dict[encounter_id]\n",
    "\n",
    "        if duration_minute > 60. * hour_threshold:\n",
    "            continue\n",
    "\n",
    "        ei = EncounterInfo(patient_id, encounter_id, encounter_timestamp,\n",
    "                           readmission)\n",
    "        if encounter_id in encounter_dict:\n",
    "            print('Duplicate encounter ID!!')\n",
    "            sys.exit(0)\n",
    "        encounter_dict[encounter_id] = ei\n",
    "        count += 1\n",
    "\n",
    "    inff.close()\n",
    "    print('')\n",
    "\n",
    "    return encounter_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8fd1fffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_admission_dx(infile, encounter_dict):\n",
    "    inff = open(infile, 'r')\n",
    "    count = 0\n",
    "    missing_eid = 0\n",
    "    for line in csv.DictReader(inff):\n",
    "        if count % 10000 == 0:\n",
    "            sys.stdout.write('%d\\r' % count)\n",
    "            sys.stdout.flush()\n",
    "\n",
    "        encounter_id = line['patientunitstayid']\n",
    "        dx_id = line['admitdxpath'].lower()\n",
    "\n",
    "        if encounter_id not in encounter_dict:\n",
    "            missing_eid += 1\n",
    "            continue\n",
    "        encounter_dict[encounter_id].dx_ids.append(dx_id)\n",
    "        count += 1\n",
    "    inff.close()\n",
    "    print('')\n",
    "    print('Admission Diagnosis without Encounter ID: %d' % missing_eid)\n",
    "\n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d6b4463d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_diagnosis(infile, encounter_dict):\n",
    "    inff = open(infile, 'r')\n",
    "    count = 0\n",
    "    missing_eid = 0\n",
    "    for line in csv.DictReader(inff):\n",
    "        if count % 10000 == 0:\n",
    "            sys.stdout.write('%d\\r' % count)\n",
    "            sys.stdout.flush()\n",
    "\n",
    "        encounter_id = line['patientunitstayid']\n",
    "        dx_id = line['diagnosisstring'].lower()\n",
    "\n",
    "        if encounter_id not in encounter_dict:\n",
    "            missing_eid += 1\n",
    "            continue\n",
    "        encounter_dict[encounter_id].dx_ids.append(dx_id)\n",
    "        count += 1\n",
    "    inff.close()\n",
    "    print('')\n",
    "    print('Diagnosis without Encounter ID: %d' % missing_eid)\n",
    "\n",
    "    return encounter_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3e1baa56",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_treatment(infile, encounter_dict):\n",
    "    inff = open(infile, 'r')\n",
    "    count = 0\n",
    "    missing_eid = 0\n",
    "\n",
    "    for line in csv.DictReader(inff):\n",
    "        if count % 10000 == 0:\n",
    "            sys.stdout.write('%d\\r' % count)\n",
    "            sys.stdout.flush()\n",
    "        encounter_id = line['patientunitstayid']\n",
    "        treatment_id = line['treatmentstring'].lower()\n",
    "        if encounter_id not in encounter_dict:\n",
    "            missing_eid += 1\n",
    "            continue\n",
    "        encounter_dict[encounter_id].treatments.append(treatment_id)\n",
    "        count += 1\n",
    "    inff.close()\n",
    "    print('')\n",
    "    print('Treatment without Encounter ID: %d' % missing_eid)\n",
    "    print('Accepted treatments: %d' % count)\n",
    "\n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "85aa0915",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_seqex(enc_dict,\n",
    "                skip_duplicate=False,\n",
    "                min_num_codes=1,\n",
    "                max_num_codes=50):\n",
    "    key_list = []\n",
    "    seqex_list = []\n",
    "    dx_str2int = {}\n",
    "    treat_str2int = {}\n",
    "    num_cut = 0\n",
    "    num_duplicate = 0\n",
    "    count = 0\n",
    "    num_dx_ids = 0\n",
    "    num_treatments = 0\n",
    "    num_unique_dx_ids = 0\n",
    "    num_unique_treatments = 0\n",
    "    min_dx_cut = 0\n",
    "    min_treatment_cut = 0\n",
    "    max_dx_cut = 0\n",
    "    max_treatment_cut = 0\n",
    "    num_readmission = 0\n",
    "\n",
    "    for _, enc in enc_dict.items():\n",
    "        if skip_duplicate:\n",
    "            if (len(enc.dx_ids) > len(set(enc.dx_ids)) or\n",
    "                    len(enc.treatments) > len(set(enc.treatments))):\n",
    "                num_duplicate += 1\n",
    "                continue\n",
    "\n",
    "        if len(set(enc.dx_ids)) < min_num_codes:\n",
    "            min_dx_cut += 1\n",
    "            continue\n",
    "\n",
    "        if len(set(enc.treatments)) < min_num_codes:\n",
    "            min_treatment_cut += 1\n",
    "            continue\n",
    "\n",
    "        if len(set(enc.dx_ids)) > max_num_codes:\n",
    "            max_dx_cut += 1\n",
    "            continue\n",
    "\n",
    "        if len(set(enc.treatments)) > max_num_codes:\n",
    "            max_treatment_cut += 1\n",
    "            continue\n",
    "\n",
    "        count += 1\n",
    "        num_dx_ids += len(enc.dx_ids)\n",
    "        num_treatments += len(enc.treatments)\n",
    "        num_unique_dx_ids += len(set(enc.dx_ids))\n",
    "        num_unique_treatments += len(set(enc.treatments))\n",
    "\n",
    "        for dx_id in enc.dx_ids:\n",
    "            if dx_id not in dx_str2int:\n",
    "                dx_str2int[dx_id] = len(dx_str2int)\n",
    "\n",
    "        for treat_id in enc.treatments:\n",
    "            if treat_id not in treat_str2int:\n",
    "                treat_str2int[treat_id] = len(treat_str2int)\n",
    "\n",
    "        seqex = tf.train.SequenceExample()\n",
    "        seqex.context.feature['patientId'].bytes_list.value.append(\n",
    "            bytes(enc.patient_id + ':' +enc.encounter_id, 'utf-8'))\n",
    "\n",
    "        if enc.readmission:\n",
    "            seqex.context.feature['label'].int64_list.value.append(1)\n",
    "            num_readmission += 1\n",
    "        else:\n",
    "            seqex.context.feature['label'].int64_list.value.append(0)\n",
    "\n",
    "        dx_ids = seqex.feature_lists.feature_list['dx_ids']\n",
    "        dx_ids.feature.add().bytes_list.value.extend(list([bytes(s, 'utf-8') for s in set(enc.dx_ids)]))\n",
    "\n",
    "        dx_int_list = [dx_str2int[item] for item in list(set(enc.dx_ids))]\n",
    "        dx_ints = seqex.feature_lists.feature_list['dx_ints']\n",
    "        dx_ints.feature.add().int64_list.value.extend(dx_int_list)\n",
    "\n",
    "        proc_ids = seqex.feature_lists.feature_list['proc_ids']\n",
    "        proc_ids.feature.add().bytes_list.value.extend(list([bytes(s, 'utf-8') for s in set(enc.treatments)]))\n",
    "\n",
    "        proc_int_list = [treat_str2int[item] for item in list(set(enc.treatments))]\n",
    "        proc_ints = seqex.feature_lists.feature_list['proc_ints']\n",
    "        proc_ints.feature.add().int64_list.value.extend(proc_int_list)\n",
    "\n",
    "        seqex_list.append(seqex)\n",
    "        key = seqex.context.feature['patientId'].bytes_list.value[0]\n",
    "        key_list.append(key)\n",
    "\n",
    "    print('Filtered encounters due to duplicate codes: %d' % num_duplicate)\n",
    "    print('Filtered encounters due to thresholding: %d' % num_cut)\n",
    "    print('Average num_dx_ids: %f' % (num_dx_ids / count))\n",
    "    print('Average num_treatments: %f' % (num_treatments / count))\n",
    "    print('Average num_unique_dx_ids: %f' % (num_unique_dx_ids / count))\n",
    "    print('Average num_unique_treatments: %f' % (num_unique_treatments / count))\n",
    "    print('Min dx cut: %d' % min_dx_cut)\n",
    "    print('Min treatment cut: %d' % min_treatment_cut)\n",
    "    print('Max dx cut: %d' % max_dx_cut)\n",
    "    print('Max treatment cut: %d' % max_treatment_cut)\n",
    "    print('Number of readmission: %d' % num_readmission)\n",
    "\n",
    "    return key_list, seqex_list, dx_str2int, treat_str2int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "05adfcd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_train_valid_test(key_list, random_seed=0):\n",
    "    train_id, val_id = model_selection.train_test_split(\n",
    "        key_list, test_size=0.2, random_state=random_seed)\n",
    "    test_id, val_id = model_selection.train_test_split(\n",
    "        val_id, test_size=0.5, random_state=random_seed)\n",
    "    return train_id, val_id, test_id\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8296f407",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_partitions(seqex_list, id_set=None):\n",
    "    total_visit = 0\n",
    "    new_seqex_list = []\n",
    "    for seqex in seqex_list:\n",
    "        if total_visit % 1000 == 0:\n",
    "            sys.stdout.write('Visit count: %d\\r' % total_visit)\n",
    "            sys.stdout.flush()\n",
    "        key = seqex.context.feature['patientId'].bytes_list.value[0]\n",
    "        if (id_set is not None and key not in id_set):\n",
    "            total_visit += 1\n",
    "            continue\n",
    "        new_seqex_list.append(seqex)\n",
    "    return new_seqex_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c45fa3ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parser_fn(serialized_example):\n",
    "    context_features_config = {\n",
    "        'patientId': tf.VarLenFeature(tf.string),\n",
    "        'label': tf.FixedLenFeature([1], tf.int64),\n",
    "    }\n",
    "    sequence_features_config = {\n",
    "        'dx_ints': tf.VarLenFeature(tf.int64),\n",
    "        'proc_ints': tf.VarLenFeature(tf.int64)\n",
    "    }\n",
    "    (batch_context, batch_sequence) = tf.io.parse_single_sequence_example(\n",
    "        serialized_example,\n",
    "        context_features=context_features_config,\n",
    "        sequence_features=sequence_features_config)\n",
    "    labels = tf.squeeze(tf.cast(batch_context['label'], tf.float32))\n",
    "    return batch_sequence, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "368df7f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Set <input_path> to where the raw eICU CSV files are located.\\nSet <output_path> to where you want the output files to be.\\n'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def tf2csr(output_path, partition, maps):\n",
    "    num_epochs = 1\n",
    "    buffer_size = 32\n",
    "    dataset = tf.data.TFRecordDataset(output_path + partition + \".tfrecord\")\n",
    "    dataset = dataset.shuffle(buffer_size)\n",
    "    dataset = dataset.repeat(num_epochs)\n",
    "    dataset = dataset.map(parser_fn, num_parallel_calls=4)\n",
    "    dataset = dataset.batch(1)\n",
    "    dataset = dataset.prefetch(16)\n",
    "    count = 0\n",
    "    np_data = []\n",
    "    np_label = []\n",
    "    for data in dataset:\n",
    "        count += 1\n",
    "        np_datum = np.zeros(sum([len(m) for m in maps]))\n",
    "        dx_pos = tf.sparse.to_dense(data[0]['dx_ints']).numpy().ravel()\n",
    "        proc_pos = tf.sparse.to_dense(data[0]['proc_ints']).numpy().ravel() + \\\n",
    "                   sum([len(m) for m in maps[:1]])\n",
    "        np_datum[dx_pos] = 1\n",
    "        np_datum[proc_pos] = 1\n",
    "        np_data.append(np_datum)\n",
    "        np_label.append(data[1].numpy()[0])\n",
    "        sys.stdout.write('%d\\r' % count)\n",
    "        sys.stdout.flush()\n",
    "    pickle.dump((csr_matrix(np.array(np_data)), np.array(np_label)), \\\n",
    "                open(output_path + partition + '_csr.pkl', 'wb'))\n",
    "\n",
    "\"\"\"Set <input_path> to where the raw eICU CSV files are located.\n",
    "Set <output_path> to where you want the output files to be.\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d018d526",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    parser = argparse.ArgumentParser(description='File path')\n",
    "    parser.add_argument('--input_path', type=str, default='.', help='input path of original dataset')\n",
    "    parser.add_argument('--output_path', type=str, default='.', help='output path of processed dataset')\n",
    "    args = parser.parse_args()\n",
    "    input_path = args.input_path\n",
    "    output_path = args.output_path\n",
    "\n",
    "    patient_file = input_path + '/patient.csv'\n",
    "    admission_dx_file = input_path + '/admissionDx.csv'\n",
    "    diagnosis_file = input_path + '/diagnosis.csv'\n",
    "    treatment_file = input_path + '/treatment.csv'\n",
    "\n",
    "    encounter_dict = {}\n",
    "    print('Processing patient.csv')\n",
    "    encounter_dict = process_patient(\n",
    "        patient_file, encounter_dict, hour_threshold=24)\n",
    "    print(len(encounter_dict))\n",
    "    print('Processing admission diagnosis.csv')\n",
    "    encounter_dict = process_admission_dx(admission_dx_file, encounter_dict)\n",
    "    print('Processing diagnosis.csv')\n",
    "    encounter_dict = process_diagnosis(diagnosis_file, encounter_dict)\n",
    "    print('Processing treatment.csv')\n",
    "    encounter_dict = process_treatment(treatment_file, encounter_dict)\n",
    "\n",
    "    key_list, seqex_list, dx_map, proc_map = build_seqex(\n",
    "        encounter_dict, skip_duplicate=False, min_num_codes=1, max_num_codes=50)\n",
    "\n",
    "    pickle.dump(dx_map, open(output_path + '/dx_map.p', 'wb'), -1)\n",
    "    pickle.dump(proc_map, open(output_path + '/proc_map.p', 'wb'), -1)\n",
    "\n",
    "    key_train, key_valid, key_test = select_train_valid_test(key_list)\n",
    "\n",
    "    train_seqex = get_partitions(seqex_list, set(key_train))\n",
    "    validation_seqex = get_partitions(seqex_list, set(key_valid))\n",
    "    test_seqex = get_partitions(seqex_list, set(key_test))\n",
    "\n",
    "    print(\"Split done.\")\n",
    "\n",
    "    with tf.io.TFRecordWriter(output_path + '/train.tfrecord') as writer:\n",
    "        for seqex in train_seqex:\n",
    "            writer.write(seqex.SerializeToString())\n",
    "\n",
    "    with tf.io.TFRecordWriter(output_path + '/validation.tfrecord') as writer:\n",
    "        for seqex in validation_seqex:\n",
    "            writer.write(seqex.SerializeToString())\n",
    "\n",
    "    with tf.io.TFRecordWriter(output_path + '/test.tfrecord') as writer:\n",
    "        for seqex in test_seqex:\n",
    "            writer.write(seqex.SerializeToString())\n",
    "\n",
    "    for partition in ['train', 'validation', 'test']:\n",
    "        tf2csr(output_path, partition, [dx_map, proc_map])\n",
    "    print('done')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
