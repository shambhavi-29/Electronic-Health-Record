{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "94fbd27d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "29c0b0d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = 'cuda'\n",
    "else:\n",
    "    device = 'cpu'\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "55f2fe1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clones(module, N):\n",
    "    return nn.ModuleList([copy.deepcopy(module) for _ in range(N)])\n",
    "\n",
    "\n",
    "def clone_params(param, N):\n",
    "    return nn.ParameterList([copy.deepcopy(param) for _ in range(N)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "958cd372",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, features, eps=1e-6):\n",
    "        super(LayerNorm, self).__init__()\n",
    "        self.a_2 = nn.Parameter(torch.ones(features))\n",
    "        self.b_2 = nn.Parameter(torch.zeros(features))\n",
    "        self.eps = eps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "60d1795b",
   "metadata": {},
   "outputs": [],
   "source": [
    " def forward(self, x):\n",
    "        mean = x.mean(-1, keepdim=True)\n",
    "        std = x.std(-1, keepdim=True)\n",
    "        return self.a_2 * (x - mean) / (std + self.eps) + self.b_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f86043ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphLayer(nn.Module):\n",
    "     def __init__(self, in_features, hidden_features, out_features, num_of_nodes,\n",
    "                 num_of_heads, dropout, alpha, concat=True):\n",
    "        super(GraphLayer, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.hidden_features = hidden_features\n",
    "        self.out_features = out_features\n",
    "        self.alpha = alpha\n",
    "        self.concat = concat\n",
    "        self.num_of_nodes = num_of_nodes\n",
    "        self.num_of_heads = num_of_heads\n",
    "        self.W = clones(nn.Linear(in_features, hidden_features), num_of_heads)\n",
    "        self.a = clone_params(nn.Parameter(torch.rand(size=(1, 2 * hidden_features)), requires_grad=True), num_of_heads)\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(out_features, out_features),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        if not concat:\n",
    "            self.V = nn.Linear(hidden_features, out_features)\n",
    "        else:\n",
    "            self.V = nn.Linear(num_of_heads * hidden_features, out_features)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.leakyrelu = nn.LeakyReLU(self.alpha)\n",
    "        if concat:\n",
    "            self.norm = LayerNorm(hidden_features)\n",
    "        else:\n",
    "            self.norm = LayerNorm(hidden_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "827dd329",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize(self):\n",
    "        for i in range(len(self.W)):\n",
    "            nn.init.xavier_normal_(self.W[i].weight.data)\n",
    "        for i in range(len(self.a)):\n",
    "            nn.init.xavier_normal_(self.a[i].data)\n",
    "        if not self.concat:\n",
    "            nn.init.xavier_normal_(self.V.weight.data)\n",
    "            nn.init.xavier_normal_(self.out_layer.weight.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f0f38dcf",
   "metadata": {},
   "outputs": [],
   "source": [
    " def attention(self, linear, a, N, data, edge):\n",
    "        data = linear(data).unsqueeze(0)\n",
    "        assert not torch.isnan(data).any()\n",
    "        \n",
    "        h = torch.cat((data[:, edge[0, :], :], data[:, edge[1, :], :]), dim=0)\n",
    "        data = data.squeeze(0)\n",
    "        \n",
    "        assert not torch.isnan(h).any()\n",
    "        \n",
    "        edge_h = torch.cat((h[0, :, :], h[1, :, :]), dim=1).transpose(0, 1)\n",
    "        \n",
    "        edge_e = torch.exp(self.leakyrelu(a.mm(edge_h).squeeze()) / np.sqrt(self.hidden_features * self.num_of_heads))\n",
    "        assert not torch.isnan(edge_e).any()\n",
    "        \n",
    "        edge_e = torch.sparse_coo_tensor(edge, edge_e, torch.Size([N, N]))\n",
    "        e_rowsum = torch.sparse.mm(edge_e, torch.ones(size=(N, 1)).to(device))\n",
    "        \n",
    "        row_check = (e_rowsum == 0)\n",
    "        e_rowsum[row_check] = 1\n",
    "        zero_idx = row_check.nonzero()[:, 0]\n",
    "        edge_e = edge_e.add(\n",
    "            torch.sparse.FloatTensor(zero_idx.repeat(2, 1), torch.ones(len(zero_idx)).to(device), torch.Size([N, N])))\n",
    "        \n",
    "        h_prime = torch.sparse.mm(edge_e, data)\n",
    "        assert not torch.isnan(h_prime).any()\n",
    "        \n",
    "        h_prime.div_(e_rowsum)\n",
    "        \n",
    "        assert not torch.isnan(h_prime).any()\n",
    "        return h_prime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e09e604d",
   "metadata": {},
   "outputs": [],
   "source": [
    " def forward(self, edge, data=None):\n",
    "        N = self.num_of_nodes\n",
    "        if self.concat:\n",
    "            h_prime = torch.cat([self.attention(l, a, N, data, edge) for l, a in zip(self.W, self.a)], dim=1)\n",
    "        else:\n",
    "            h_prime = torch.stack([self.attention(l, a, N, data, edge) for l, a in zip(self.W, self.a)], dim=0).mean(\n",
    "                dim=0)\n",
    "        h_prime = self.dropout(h_prime)\n",
    "        if self.concat:\n",
    "            return F.elu(self.norm(h_prime))\n",
    "        else:\n",
    "            return self.V(F.relu(self.norm(h_prime)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "58b8deca",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VariationalGNN(nn.Module):\n",
    "\n",
    "    def __init__(self, in_features, out_features, num_of_nodes, n_heads, n_layers,\n",
    "                 dropout, alpha, variational=True, none_graph_features=0, concat=True):\n",
    "        super(VariationalGNN, self).__init__()\n",
    "        self.variational = variational\n",
    "        self.num_of_nodes = num_of_nodes + 1 - none_graph_features\n",
    "        self.embed = nn.Embedding(self.num_of_nodes, in_features, padding_idx=0)\n",
    "        self.in_att = clones(\n",
    "            GraphLayer(in_features, in_features, in_features, self.num_of_nodes,\n",
    "                       n_heads, dropout, alpha, concat=True), n_layers)\n",
    "        self.out_features = out_features\n",
    "        self.out_att = GraphLayer(in_features, in_features, out_features, self.num_of_nodes,\n",
    "                                  n_heads, dropout, alpha, concat=False)\n",
    "        self.n_heads = n_heads\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.parameterize = nn.Linear(out_features, out_features * 2)\n",
    "        self.out_layer = nn.Sequential(\n",
    "            nn.Linear(out_features, out_features),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(out_features, 1))\n",
    "        self.none_graph_features = none_graph_features\n",
    "        if none_graph_features > 0:\n",
    "            self.features_ffn = nn.Sequential(\n",
    "                nn.Linear(none_graph_features, out_features//2),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(dropout))\n",
    "            self.out_layer = nn.Sequential(\n",
    "                nn.Linear(out_features + out_features//2, out_features),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(dropout),\n",
    "                nn.Linear(out_features, 1))\n",
    "        for i in range(n_layers):\n",
    "            self.in_att[i].initialize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "783a77a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_to_edges(self, data):\n",
    "        length = data.size()[0]\n",
    "        nonzero = data.nonzero()\n",
    "        if nonzero.size()[0] == 0:\n",
    "            return torch.LongTensor([[0], [0]]), torch.LongTensor([[length + 1], [length + 1]])\n",
    "        if self.training:\n",
    "            mask = torch.rand(nonzero.size()[0])\n",
    "            mask = mask > 0.05\n",
    "            nonzero = nonzero[mask]\n",
    "            if nonzero.size()[0] == 0:\n",
    "                return torch.LongTensor([[0], [0]]), torch.LongTensor([[length + 1], [length + 1]])\n",
    "        nonzero = nonzero.transpose(0, 1) + 1\n",
    "        lengths = nonzero.size()[1]\n",
    "        input_edges = torch.cat((nonzero.repeat(1, lengths),\n",
    "                                 nonzero.repeat(lengths, 1).transpose(0, 1)\n",
    "                                 .contiguous().view((1, lengths ** 2))), dim=0)\n",
    "\n",
    "        nonzero = torch.cat((nonzero, torch.LongTensor([[length + 1]]).to(device)), dim=1)\n",
    "        lengths = nonzero.size()[1]\n",
    "        output_edges = torch.cat((nonzero.repeat(1, lengths),\n",
    "                                  nonzero.repeat(lengths, 1).transpose(0, 1)\n",
    "                                  .contiguous().view((1, lengths ** 2))), dim=0)\n",
    "        return input_edges.to(device), output_edges.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c3a617d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reparameterise(self, mu, logvar):\n",
    "        if self.training:\n",
    "            std = logvar.mul(0.5).exp_()\n",
    "            eps = std.data.new(std.size()).normal_()\n",
    "            return eps.mul(std).add_(mu)\n",
    "        else:\n",
    "            return mu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dc039d96",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoder_decoder(self, data):\n",
    "        N = self.num_of_nodes\n",
    "        input_edges, output_edges = self.data_to_edges(data)\n",
    "        h_prime = self.embed(torch.arange(N).long().to(device))\n",
    "        for attn in self.in_att:\n",
    "            h_prime = attn(input_edges, h_prime)\n",
    "        if self.variational:\n",
    "            h_prime = self.parameterize(h_prime).view(-1, 2, self.out_features)\n",
    "            h_prime = self.dropout(h_prime)\n",
    "            mu = h_prime[:, 0, :]\n",
    "            logvar = h_prime[:, 1, :]\n",
    "            h_prime = self.reparameterise(mu, logvar)\n",
    "            mu = mu[data, :]\n",
    "            logvar = logvar[data, :]\n",
    "        h_prime = self.out_att(output_edges, h_prime)\n",
    "        if self.variational:\n",
    "            return h_prime[-1], 0.5 * torch.sum(logvar.exp() - logvar - 1 + mu.pow(2)) / mu.size()[0]\n",
    "        else:\n",
    "            return h_prime[-1], torch.tensor(0.0).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b11743c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(self, data):\n",
    "        # Concate batches\n",
    "        batch_size = data.size()[0]\n",
    "        # In eicu data the first feature whether have be admitted before is not included in the graph\n",
    "        if self.none_graph_features == 0:\n",
    "            outputs = [self.encoder_decoder(data[i, :]) for i in range(batch_size)]\n",
    "            return self.out_layer(F.relu(torch.stack([out[0] for out in outputs]))), \\\n",
    "                   torch.sum(torch.stack([out[1] for out in outputs]))\n",
    "        else:\n",
    "            outputs = [(data[i, :self.none_graph_features],\n",
    "                        self.encoder_decoder(data[i, self.none_graph_features:])) for i in range(batch_size)]\n",
    "            return self.out_layer(F.relu(\n",
    "                torch.stack([torch.cat((self.features_ffn(torch.FloatTensor([out[0]]).to(device)), out[1][0]))\n",
    "                             for out in outputs]))), \\\n",
    "                   torch.sum(torch.stack([out[1][1] for out in outputs]), dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da930ba9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
