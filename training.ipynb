{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0bb0bbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "from torch.utils.data import DataLoader\n",
    "from collections import Counter\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "from model import VariationalGNN\n",
    "from utils import train, evaluate, EHRData, collate_fn\n",
    "import os\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4dc279ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = 'cuda'\n",
    "else:\n",
    "    device = 'cpu'\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0a13b79e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    parser = argparse.ArgumentParser(description='configuraitons')\n",
    "    parser.add_argument('--result_path', type=str, default='.', help='output path of model checkpoints')\n",
    "    parser.add_argument('--data_path', type=str, default='./mimc', help='input path of processed dataset')\n",
    "    parser.add_argument('--embedding_size', type=int, default=256, help='embedding size')\n",
    "    parser.add_argument('--num_of_layers', type=int, default=2, help='number of graph layers')\n",
    "    parser.add_argument('--num_of_heads', type=int, default=1, help='number of attention heads')\n",
    "    parser.add_argument('--lr', type=float, default=1e-4, help='learning rate')\n",
    "    parser.add_argument('--batch_size', type=int, default=32, help='batch_size')\n",
    "    parser.add_argument('--dropout', type=float, default=0.4, help='dropout')\n",
    "    parser.add_argument('--reg', type=str, default=\"True\", help='regularization')\n",
    "    parser.add_argument('--lbd', type=int, default=1.0, help='regularization')\n",
    "\n",
    "    args = parser.parse_args()\n",
    "    result_path = args.result_path\n",
    "    data_path = args.data_path\n",
    "    in_feature = args.embedding_size\n",
    "    out_feature =args.embedding_size\n",
    "    n_layers = args.num_of_layers - 1\n",
    "    lr = args.lr\n",
    "    args.reg = (args.reg == \"True\")\n",
    "    n_heads = args.num_of_heads\n",
    "    dropout = args.dropout\n",
    "    alpha = 0.1\n",
    "    BATCH_SIZE = args.batch_size\n",
    "    number_of_epochs = 50\n",
    "    eval_freq = 1000\n",
    "\n",
    "    \n",
    "    train_x, train_y = pickle.load(open(data_path + 'train_csr.pkl', 'rb'))\n",
    "    val_x, val_y = pickle.load(open(data_path + 'validation_csr.pkl', 'rb'))\n",
    "    test_x, test_y = pickle.load(open(data_path + 'test_csr.pkl', 'rb'))\n",
    "    train_upsampling = np.concatenate((np.arange(len(train_y)), np.repeat(np.where(train_y == 1)[0], 1)))\n",
    "    train_x = train_x[train_upsampling]\n",
    "    train_y = train_y[train_upsampling]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bb8be7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "s = datetime.now().strftime('%Y%m%d%H%M%S')\n",
    "result_root = '%s/lr_%s-input_%s-output_%s-dropout_%s'%(result_path, lr, in_feature, out_feature, dropout)\n",
    "if not os.path.exists(result_root):\n",
    "    os.mkdir(result_root)\n",
    "    \n",
    "        \n",
    "for handler in logging.root.handlers[:]:\n",
    "    logging.root.removeHandler(handler)\n",
    "    \n",
    "        \n",
    "logging.basicConfig(filename='%s/train.log' % result_root, format='%(asctime)s %(message)s', level=logging.INFO)\n",
    "logging.info(\"Time:%s\" %(s))\n",
    "\n",
    "\n",
    "num_of_nodes = train_x.shape[1] + 1\n",
    "device_ids = range(torch.cuda.device_count())\n",
    "   \n",
    "model = VariationalGNN(in_feature, out_feature, num_of_nodes, n_heads, n_layers,\n",
    "                           dropout=dropout, alpha=alpha, variational=args.reg, none_graph_features=0).to(device)\n",
    "model = nn.DataParallel(model, device_ids=device_ids)\n",
    "val_loader = DataLoader(dataset=EHRData(val_x, val_y), batch_size=BATCH_SIZE,\n",
    "                            collate_fn=collate_fn, num_workers=torch.cuda.device_count(), shuffle=False)\n",
    "optimizer = optim.Adam([p for p in model.parameters() if p.requires_grad], lr=lr, weight_decay=1e-8)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "104a44bf",
   "metadata": {},
   "outputs": [],
   "source": [
    " for epoch in range(number_of_epochs):\n",
    "        print(\"Learning rate:{}\".format(optimizer.param_groups[0]['lr']))\n",
    "        ratio = Counter(train_y)\n",
    "        train_loader = DataLoader(dataset=EHRData(train_x, train_y), batch_size=BATCH_SIZE,\n",
    "                                  collate_fn=collate_fn, num_workers=torch.cuda.device_count(), shuffle=True)\n",
    "        pos_weight = torch.ones(1).float().to(device) * (ratio[True] / ratio[False])\n",
    "        criterion = nn.BCEWithLogitsLoss(reduction=\"sum\", pos_weight=pos_weight)\n",
    "        t = tqdm(iter(train_loader), leave=False, total=len(train_loader))\n",
    "        model.train()\n",
    "        total_loss = np.zeros(3)\n",
    "        for idx, batch_data in enumerate(t):\n",
    "            loss, kld, bce = train(batch_data, model, optimizer, criterion, args.lbd, 5)\n",
    "            total_loss += np.array([loss, bce, kld])\n",
    "            if idx % eval_freq == 0 and idx > 0:\n",
    "                torch.save(model.state_dict(), \"{}/parameter{}_{}\".format(result_root, epoch, idx))\n",
    "                val_auprc, _ = evaluate(model, val_loader, len(val_y))\n",
    "                logging.info('epoch:%d AUPRC:%f; loss: %.4f, bce: %.4f, kld: %.4f' %\n",
    "                             (epoch + 1, val_auprc, total_loss[0]/idx, total_loss[1]/idx, total_loss[2]/idx))\n",
    "                print('epoch:%d AUPRC:%f; loss: %.4f, bce: %.4f, kld: %.4f' %\n",
    "                      (epoch + 1, val_auprc, total_loss[0]/idx, total_loss[1]/idx, total_loss[2]/idx))\n",
    "            if idx % 50 == 0 and idx > 0:\n",
    "                t.set_description('[epoch:%d] loss: %.4f, bce: %.4f, kld: %.4f' %\n",
    "                                  (epoch + 1, total_loss[0]/idx, total_loss[1]/idx, total_loss[2]/idx))\n",
    "                t.refresh()\n",
    "        scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d7cde4c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
